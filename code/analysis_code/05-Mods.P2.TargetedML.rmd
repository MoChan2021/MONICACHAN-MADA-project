---
title: "Random Forest- Targeted"
author: "MYC"
date: "11/24/2021"
output: html_document
---
Interesting results from the P2 Random Forest ML application, Try more targeted variables 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Helper packages
library(tidyverse) # for data manipulation & graphics suite
library(here)      # for easy specification of relative locations
library(tidymodels)# for modeling process suite  
library(caret)     # for resampling and model training
library(rpart)
library(rpart.plot)
library(glmnet)
```
Path to data
```{r Data}
#note the use of the here() package and not absolute paths
data_location <- here::here("data","processed_data","P2-20211029.rds")
#read and load data. 
```
Import P2 data
```{r}
P2<- readRDS(data_location)
```

Add to the data -- Fix so we add a categorical aspect to Ct values- simplicity make Detected/Yes (< 37.9) and Undetected/No (>38)

Compare only dilutions that were -1.


```{r}
P2.minus1<-
  P2%>%
  filter(Tool!="B")%>% #remove B because this is actually a -3 dilution. Looking specifically at -1, keep SD range. 
  mutate(
# if greater than ct 38, this is a negative detection, thus less 38 is a positive in detection
    Ct.Cat = ifelse(Ct >=38,"No","Yes"), 
# There are some NAs, they are undetectable, change only in the Ct.Cat to No as a factor    
    Ct.Cat=ifelse(is.na(Ct.Cat), "No",Ct.Cat),
# These are factors so mutate the chr to fctrs
    Ct.Cat=factor(Ct.Cat)
    )%>%
#Remove/adjust so there are no character based factors
    select(PCR, RH,TEMP, Method, Tool, Input, Detector, Ct, Ct.Cat,Dilution)%>%
  na.omit()

summary(P2.minus1)  
```

Set Seed for reproducibility
```{r}
# Fix the random numbers by setting the seed 
set.seed(123)
# This enables the analysis to be reproducible when random numbers are used
```

1. Data splitting into training and testing sets (Random sampling)
  1. Training
  2. Testing
  
```{r}
# Put 3/4 of the data into the training set; this leaves 1/4 of the data to be used to test 
data_split <- initial_split(P2.minus1, prop = 3/4)

## Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

```{r}
#View the train and test data
glimpse(train_data)
glimpse(test_data)
```

2. Creating Models

#  Ct value or Detection as function of method and inital dilution 
Detection ~ Method + Dilution

# Ct value or detection for all precitcors (short hand for all predictors)
Detection~., data=P2.minus1

2.1. Many engines Examples
```{r}
# fit linear model
lm_lm    <- lm(Ct ~ ., data = P2.minus1)
# fit general linear model
lm_glm   <- glm(Ct ~ ., data = P2.minus1, 
                family = gaussian)
#########
#<atempt to use later>
# meta engine (aggregator) method=<method name>; see parsnips
#lm_caret <- train(Ct ~ ., data = P2.Plus.Cat, 
#                 method = "glm")
```

2.4. Resampling - CV
```{r }
# create CV object from training data
cv_data <- rsample::vfold_cv(train_data, v = 5, repeats = 5, strata = 'Ct')

print(cv_data)

############## below are similar functions but need to try

#apply CV in ML algorithm to process the ML model to each resample
vfold_cv(P2.minus1, v=5)

#bootstrapping- random sample of data with replacment
bootstraps(P2.minus1, v=5)
```
Preprocessing
Using parsnips to create a recipe for fitting the model
No standardization
adding dummy variables are added
```{r}
fit_recipe <- 
# set a recipe
  recipe(Ct ~., data = P2.minus1) %>%
# code all categorical variables as dummy variables
  step_dummy(all_nominal()) 

print(fit_recipe)
```
Null Model
Ct is a continuions outcome of numbers-- need to use RMSE as performance metric.
```{r}
RMSE_null_train <- sqrt(sum( (train_data$Ct - mean(train_data$Ct))^2 )/nrow(train_data))

RMSE_null_test <- sqrt(sum( (test_data$Ct - mean(test_data$Ct))^2 )/nrow(test_data))

print(RMSE_null_train)

#predict(lm_lm, train_data=Ct)
```

```{r}
rf_model <- rand_forest() %>%
  set_args(mtry = tune(),     
    trees = tune(),
    min_n = tune()
  ) %>%
  # select the engine/package that underlies the model
  set_engine("ranger",
             num.threads = 18, #for some reason for RF, we need to set this in the engine too
             importance = "permutation") %>%
  # choose either the continuous regression or binary classification mode
  set_mode("regression")   
```

```{r}
#workflow
rf_wf <- workflow() %>%
  add_model(rf_model) %>% 
  add_recipe(fit_recipe)
```

```{r}
#tuning grid
rf_grid  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )

# tune the model, optimizing RMSE
rf_tune_res <- rf_wf %>%
  tune_grid(
            resamples = cv_data, #CV object
            grid = rf_grid, # grid of values to try
            metrics = metric_set(rmse) 
  )
```

```{r}
#see a plot of performance for different tuning parameters
rf_tune_res %>% autoplot()
```
```{r}
# get the tuned model that performs best 
best_rf <- rf_tune_res %>%  select_best(metric = "rmse")

# finalize workflow with best model
best_rf_wf <- rf_wf %>% finalize_workflow(best_rf)

# fitting best performing model
best_rf_fit <- best_rf_wf %>% 
  fit(data = train_data)
rf_pred <- predict(best_rf_fit, train_data)
```

```{r}
#pull out the fit object
x <- best_rf_fit$fit$fit$fit
#plot variable importance
P2RFBestFitminus1<-vip::vip(x, num_features = 20)
print(P2RFBestFitminus1)
```
Interesting so Dilution is obviously more important followed by Input. It's more surprizing that the Method is one of the least important in this comparison. Infact the temperature and relative humidity are more impractically thatn the method.
```{r warning=FALSE}
#save figure
figure_file = here("results","P2-RF-BestFit-minus1.png")
ggsave(filename = figure_file, plot=P2RFBestFitminus1) 
```

```{r}
#predicted versus observed
plot(rf_pred$.pred,train_data$Ct, xlim =c(10,50), ylim=c(10,50))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
```
```{r}
#residuals
plot(rf_pred$.pred-train_data$Ct)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```
```{r}
rf_perfomance <- rf_tune_res %>% show_best(n = 5)
print(rf_perfomance)
```

# Try to see if Best Tree can show us something


Tree model
```{r}
tree_model <-  decision_tree() %>% 
  set_args( cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")      
```
Workflow
```{r}
#set workflow
tree_wf <- workflow() %>%
  add_model(tree_model) %>%
  add_recipe(fit_recipe)
```
Tree Model
```{r}
#tuning grid
tree_grid <- dials::grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)
#tune the model
tree_tune_res <- tree_wf %>% 
  tune::tune_grid(
    resamples = cv_data,
    grid = tree_grid,
    metrics = yardstick::metric_set(rmse) 
  )
```

```{r}
#see a plot of performance for different tuning parameters
tree_tune_res %>% autoplot()
```
```{r}
# get the tuned model that performs best 
best_tree <- tree_tune_res %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_tree_wf <- tree_wf %>% finalize_workflow(best_tree)
# fitting best performing model
best_tree_fit <- best_tree_wf %>% 
                 fit(data = train_data)
#predicting outcomes for final model
tree_pred <- predict(best_tree_fit, train_data)
```

```{r}
rpart.plot(extract_fit_parsnip(best_tree_fit)$fit)
```