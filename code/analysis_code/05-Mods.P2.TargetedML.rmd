---
title: "Random Forest- Targeted"
author: "MYC"
date: "11/24/2021"
output: html_document
---
Interesting results from the P2 Random Forest ML application, Try more targeted variables 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Helper packages
library(tidyverse) # for data manipulation & graphics suite
library(here)      # for easy specification of relative locations
library(tidymodels)# for modeling process suite  
library(caret)     # for resampling and model training
library(rpart)
library(rpart.plot)
library(glmnet)
```
Path to data
```{r Data}
#note the use of the here() package and not absolute paths
data_location <- here::here("data","processed_data","P2-20211029.rds")
#read and load data. 
```
Import P2 data
```{r}
P2<- readRDS(data_location)
```

Add to the data -- Fix so we add a categorical aspect to Ct values- simplicity make Detected/Yes (< 37.9) and Undetected/No (>38)

Compare only dilutions that were -1.


```{r}
P2.minus1<-
  P2%>%
  filter(Tool!="B")%>% #remove B because this is actually a -3 dilution. Looking specifically at -1, keep SD range.
  mutate(
# if greater than ct 38, this is a negative detection, thus less 38 is a positive in detection
    Ct.Cat = ifelse(Ct >=38,"No","Yes"), 
# There are some NAs, they are undetectable, change only in the Ct.Cat to No as a factor    
    Ct.Cat=ifelse(is.na(Ct.Cat), "No",Ct.Cat),
# These are factors so mutate the chr to fctrs
    Ct.Cat=factor(Ct.Cat)
    )%>%
#Remove/adjust so there are no character based factors
    select(RH,TEMP, Method, Tool, Input, Detector, Ct, Ct.Cat,Dilution)%>%
  na.omit()

summary(P2.minus1)  
```

Set Seed for reproducibility
```{r}
# Fix the random numbers by setting the seed 
set.seed(123)
# This enables the analysis to be reproducible when random numbers are used
```

1. Data splitting into training and testing sets (Random sampling)
  1. Training
  2. Testing
  
```{r}
# Put 3/4 of the data into the training set; this leaves 1/4 of the data to be used to test 
data_split2 <- initial_split(P2.minus1, prop = 3/4)

## Create data frames for the two sets:
train_data2 <- training(data_split2)
test_data2  <- testing(data_split2)
```

```{r}
#View the train and test data
glimpse(train_data2)
glimpse(test_data2)

table(train_data2$Method)%>%
  prop.table()
table(test_data2$Method)%>%
  prop.table()
```

2. Creating Models

#  Ct value or Detection as function of method and inital dilution 
Detection ~ Method + Dilution

# Ct value or detection for all precitcors (short hand for all predictors)
Detection~., data=P2.minus1

2.1. Many engines Examples
```{r}
# fit linear model
lm_lm    <- lm(Ct ~ ., data = P2.minus1)
# fit general linear model
lm_glm   <- glm(Ct ~ ., data = P2.minus1, 
                family = gaussian)
#########
#<atempt to use later>
# meta engine (aggregator) method=<method name>; see parsnips
lm_caret <- train(Ct ~ Method + Dilution + Input, data = P2.minus1, 
                 method = "glm")
```

2.4. Resampling - CV
```{r }
# create bootstrap resample object from training data
cv_data_boots <- bootstraps(train_data2, v=5)
print(cv_data_boots)
```
Preprocessing
Using parsnips to create a recipe for fitting the model
No standardization
adding dummy variables are added
```{r}
fit_recipe2 <- 
# set a recipe specific to the Dilution and Input
  recipe(Ct ~Dilution+Input+Method, data = P2.minus1) %>%
# code all categorical variables as dummy variables
  step_dummy(all_nominal()) 

print(fit_recipe2)
```
Null Model
Ct is a continuions outcome of numbers-- need to use RMSE as performance metric.
```{r}
RMSE_null_train2 <- sqrt(sum( (train_data2$Ct - mean(train_data2$Ct))^2 )/nrow(train_data2))

RMSE_null_test2 <- sqrt(sum( (test_data2$Ct - mean(test_data2$Ct))^2 )/nrow(test_data2))

print(RMSE_null_train2)

predict(lm_glm, train_data=Ct)
```

```{r}
rf_model2 <- rand_forest() %>%
  set_args(mtry = tune(),     
    trees = tune(),
    min_n = tune()
  ) %>%
  # select the engine/package that underlies the model
  set_engine("ranger",
             num.threads = 18, #for some reason for RF, we need to set this in the engine too
             importance = "permutation") %>%
  # choose either the continuous regression or binary classification mode
  set_mode("regression")   
```

```{r}
#workflow
rf_wf2 <- workflow() %>%
  add_model(rf_model2) %>% 
  add_recipe(fit_recipe2)
```
Try bootstrapping in CV workflow
```{r}
#tuning grid
rf_grid2  <- expand.grid(mtry = c(3, 4, 5, 6), min_n = c(40,50,60), trees = c(500,1000)  )

# tune the model, optimizing RMSE
rf_tune_res2 <- rf_wf2 %>%
  tune_grid(
            resamples = cv_data_boots, #CV object
            grid = rf_grid2, # grid of values to try
            metrics = metric_set(rmse) 
  )
```

```{r}
#see a plot of performance for different tuning parameters
rf_tune_res2 %>% autoplot()
```
```{r}
# get the tuned model that performs best 
best_rf2 <- rf_tune_res2 %>%  select_best(metric = "rmse")

# finalize workflow with best model
best_rf_wf2 <- rf_wf2 %>% finalize_workflow(best_rf2)

# fitting best performing model
best_rf_fit2 <- best_rf_wf2 %>% 
  fit(data = train_data2)
rf_pred2 <- predict(best_rf_fit2, train_data2)
```

```{r}
#pull out the fit object
x2 <- best_rf_fit2$fit$fit$fit
#plot variable importance
P2RFBestFitminus1<-vip::vip(x2, num_features = 10, geom="point", aesthetics = list(color = "red", shape = 17, size = 4))
print(P2RFBestFitminus1)
```
Interesting so Dilution is obviously more important followed by Input. It's more surprizing that the Method is one of the least important in this comparison. Infact the temperature and relative humidity are more impractically thatn the method.
```{r warning=FALSE}
#save figure
figure_file = here("results","P2-RF-BestFit-minus1.png")
ggsave(filename = figure_file, plot=P2RFBestFitminus1) 
```

```{r}
#predicted versus observed
plot(rf_pred2$.pred,train_data2$Ct)
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
```
```{r}
#residuals
plot(rf_pred2$.pred-train_data2$Ct)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```
```{r}
rf_perfomance2 <- rf_tune_res2 %>% show_best(n = 5)
print(rf_perfomance2)
```

# Try to see if Best Tree can show us something


Tree model
```{r}
tree_model2 <-  decision_tree() %>% 
  set_args( cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")      
```
Workflow
```{r}
#set workflow
tree_wf2 <- workflow() %>%
  add_model(tree_model2) %>%
  add_recipe(fit_recipe2)
```
Tree Model
```{r}
#tuning grid
tree_grid2 <- dials::grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)
#tune the model
tree_tune_res2 <- tree_wf2 %>% 
  tune::tune_grid(
    resamples = cv_data_boots,
    grid = tree_grid2,
    metrics = yardstick::metric_set(rmse) 
  )
```

```{r}
#see a plot of performance for different tuning parameters
tree_tune_res2 %>% autoplot()
```
```{r}
# get the tuned model that performs best 
best_tree2 <- tree_tune_res2 %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_tree_wf2 <- tree_wf2 %>% finalize_workflow(best_tree2)
# fitting best performing model
best_tree_fit2 <- best_tree_wf2 %>% 
                 fit(data = train_data2)
#predicting outcomes for final model
tree_pred2 <- predict(best_tree_fit2, train_data2)
```

```{r}
minusTree<-
  rpart.plot(extract_fit_parsnip(best_tree_fit2)$fit,type = 5, tweak = 1.2)

print(minusTree)
```
Each node shows: predicted values, and percentage of observations in node.
So if the dilution is still the lead in the effects of the Ct. - Makes sense as that's the spiking/manipulating level 
If the dilution is greater than -1.5, effects of the input factor strongly in the Ct recovery, with a little input from the method if the input is less than 650uL 
