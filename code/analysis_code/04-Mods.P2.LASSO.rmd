---
title: "LASSO"
author: "MYC"
date: "11/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Helper packages
library(tidyverse) # for data manipulation & graphics suite
library(here)      # for easy specification of relative locations
library(tidymodels)# for modeling process suite  
library(caret)     # for resampling and model training
library(rpart)
library(rpart.plot)
library(glmnet)
```
Path to data
```{r Data}
#note the use of the here() package and not absolute paths
data_location <- here::here("data","processed_data","P2-20211029.rds")
#read and load data. 
```
Import P2 data
```{r}
P2<- readRDS(data_location)
```

Add to the data -- Fix so we add a catagorical aspect to Ct values- simplicity make Detected/Yes (< 37.9) and Undetected/No (>38)

Bonus- try to figure out how to change into 3 levels?
```{r}
P2.Plus.Cat<-
  P2%>%
  mutate(
# if greater than ct 38, this is a negative detection, thus less 38 is a positive in detection
    Ct.Cat = ifelse(Ct >=38,"No","Yes"), 
# There are some NAs, they are undetectable, change only in the Ct.Cat to No as a factor    
    Ct.Cat=ifelse(is.na(Ct.Cat), "No",Ct.Cat),
# These are factors so mutate the chr to fctrs
    Ct.Cat=factor(Ct.Cat)
    )%>%
#Remove/adjust so there are no character based factors
    select(PCR, RH,TEMP, Method, Tool, Input, Detector, Ct, Ct.Cat)%>%
  na.omit()

summary(P2.Plus.Cat)  
```

Set Seed for reproducibility
```{r}
# Fix the random numbers by setting the seed 
set.seed(123)
# This enables the analysis to be reproducible when random numbers are used
```

1. Data splitting into training and testing sets (Random sampling)
  1. Training
  2. Testing
  
```{r}
# Put 3/4 of the data into the training set; this leaves 1/4 of the data to be used to test 
data_split <- initial_split(P2.Plus.Cat, prop = 3/4)

## Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

```{r}
#View the train and test data
glimpse(train_data)
glimpse(test_data)
```

2. Creating Models

#  Ct value or Detection as function of method and inital dilution 
Detection ~ Model + Dilution

# Ct value or detection for all precitcors (short hand for all predictors)
Detection~., data=P2.Plus.Cat

2.1. Many engines Examples
```{r}
# fit linear model
lm_lm    <- lm(Ct ~ ., data = P2.Plus.Cat)
# fit general linear model
lm_glm   <- glm(Ct ~ ., data = P2.Plus.Cat, 
                family = gaussian)
#########
#<atempt to use later>
# meta engine (aggregator) method=<method name>; see parsnips
#lm_caret <- train(Ct ~ ., data = P2.Plus.Cat, 
#                 method = "glm")
```

2.4. Resampling - CV
```{r }
# create CV object from training data
cv_data <- rsample::vfold_cv(train_data, v = 5, repeats = 5, strata = 'Ct')

print(cv_data)

############## below are similar functions but need to try

#apply CV in ML algorithm to process the ML model to each resample
vfold_cv(P2.Plus.Cat, v=5)

#bootstrapping- random sample of data with replacment
bootstraps(P2.Plus.Cat, v=5)
```
Preprocessing
Using parsnims to create a recipe for fitting the model
No standardization
adding dummy variables are added
```{r}
fit_recipe <- 
# set a recipe
  recipe(Ct ~ ., data = P2.Plus.Cat) %>%
# code all categorical variables as dummy variables
  step_dummy(all_nominal()) 

print(fit_recipe)
```
Null Model
Ct is a continuions outcome of numbers-- need to use RMSE as performance metric.
```{r}
RMSE_null_train <- sqrt(sum( (train_data$Ct - mean(train_data$Ct))^2 )/nrow(train_data))

RMSE_null_test <- sqrt(sum( (test_data$Ct - mean(test_data$Ct))^2 )/nrow(test_data))

print(RMSE_null_train)

#predict(lm_lm, train_data=Ct)
```

# LASSO Linear model

## Set up
```{r}
#model
lasso_model <- linear_reg() %>%
  set_mode("regression") %>%           
  set_engine("glmnet") %>%
  set_args(penalty = tune(), mixture = 1) #mixture = 1 means we use the LASSO model

#workflow
lasso_wf <- workflow() %>%
  add_model(lasso_model) %>% 
  add_recipe(fit_recipe)
```

## Tuning
```{r}
#tuning grid
lasso_reg_grid <- tibble(penalty = 10^seq(-3, 0, length.out = 30))

#tune model
lasso_tune_res <- lasso_wf %>% 
  tune_grid(resamples = cv_data,
            grid = lasso_reg_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse)
            )
```

## Evaluation

```{r}
#see a plot of performance for different tuning parameters
lasso_tune_res %>% autoplot()
```

#
```{r}
# get the tuned model that performs best 
best_lasso <- lasso_tune_res %>%  select_best(metric = "rmse")

# finalize workflow with best model
best_lasso_wf <- lasso_wf %>% finalize_workflow(best_lasso)

# fitting best performing model
best_lasso_fit <- best_lasso_wf %>% 
  fit(data = train_data)
lasso_pred <- predict(best_lasso_fit, train_data)
```

```{r}
x <- best_lasso_fit$fit$fit$fit
plot(x, "lambda")
```
Variables that are part of the best-fit LASSO model, i.e. those that have a non-zero coefficient.
```{r}
tidy(extract_fit_parsnip(best_lasso_fit)) %>% filter(estimate != 0)
```

Plotting observed/predicted and residuals.
```{r}
#predicted versus observed
plot(lasso_pred$.pred,train_data$Ct, xlim =c(15,50), ylim=c(15,50))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall
```
```{r}
#residuals
plot(lasso_pred$.pred-train_data$Ct)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```
Looks decent, the points are along the red lines in the plot

## Model Performance
```{r}
lasso_perfomance <- lasso_tune_res %>% show_best(n = 5)
print(lasso_perfomance)
```

