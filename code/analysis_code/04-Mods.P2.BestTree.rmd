---
title: "Best Tree"
author: "MYC"
date: "11/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Helper packages
library(tidyverse) # for data manipulation & graphics suite
library(here)      # for easy specification of relative locations
library(tidymodels)# for modeling process suite  
library(caret)     # for resampling and model training
library(rpart)
library(rpart.plot)
library(glmnet)
```
Path to data
```{r Data}
#note the use of the here() package and not absolute paths
data_location <- here::here("data","processed_data","P2-20211029.rds")
#read and load data. 
```
Import P2 data
```{r}
P2<- readRDS(data_location)
```

Add to the data -- Fix so we add a catagorical aspect to Ct values- simplicity make Detected/Yes (< 37.9) and Undetected/No (>38)

Bonus- try to figure out how to change into 3 levels?
```{r}
P2.Plus.Cat<-
  P2%>%
  mutate(
# if greater than ct 38, this is a negative detection, thus less 38 is a positive in detection
    Ct.Cat = ifelse(Ct >=38,"No","Yes"), 
# There are some NAs, they are undetectable, change only in the Ct.Cat to No as a factor    
    Ct.Cat=ifelse(is.na(Ct.Cat), "No",Ct.Cat),
# These are factors so mutate the chr to fctrs
    Ct.Cat=factor(Ct.Cat)
    )%>%
#Remove/adjust so there are no character based factors
    select(PCR, RH,TEMP, Method, Tool, Input, Detector, Ct, Ct.Cat)%>%
  na.omit()

summary(P2.Plus.Cat)  
```

Set Seed for reproducibility
```{r}
# Fix the random numbers by setting the seed 
set.seed(123)
# This enables the analysis to be reproducible when random numbers are used
```

1. Data splitting into training and testing sets (Random sampling)
  1. Training
  2. Testing
  
```{r}
# Put 3/4 of the data into the training set; this leaves 1/4 of the data to be used to test 
data_split <- initial_split(P2.Plus.Cat, prop = 3/4)

## Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

```{r}
#View the train and test data
glimpse(train_data)
glimpse(test_data)
```

2. Creating Models

#  Ct value or Detection as function of method and inital dilution 
Detection ~ Model + Dilution

# Ct value or detection for all precitcors (short hand for all predictors)
Detection~., data=P2.Plus.Cat

2.1. Many engines Examples
```{r}
# fit linear model
lm_lm    <- lm(Ct ~ ., data = P2.Plus.Cat)
# fit general linear model
lm_glm   <- glm(Ct ~ ., data = P2.Plus.Cat, 
                family = gaussian)
#########
#<atempt to use later>
# meta engine (aggregator) method=<method name>; see parsnips
#lm_caret <- train(Ct ~ ., data = P2.Plus.Cat, 
#                 method = "glm")
```

2.4. Resampling - CV
```{r }
# create CV object from training data
cv_data <- rsample::vfold_cv(train_data, v = 5, repeats = 5, strata = 'Ct')

print(cv_data)

############## below are similar functions but need to try

#apply CV in ML algorithm to process the ML model to each resample
vfold_cv(P2.Plus.Cat, v=5)

#bootstrapping- random sample of data with replacment
bootstraps(P2.Plus.Cat, v=5)
```
Preprocessing
Using parsnims to create a recipe for fitting the model
No standardization
adding dummy variables are added
```{r}
fit_recipe <- 
# set a recipe
  recipe(Ct ~ ., data = P2.Plus.Cat) %>%
# code all categorical variables as dummy variables
  step_dummy(all_nominal()) 

print(fit_recipe)
```
Null Model
Ct is a continuions outcome of numbers-- need to use RMSE as performance metric.
```{r}
RMSE_null_train <- sqrt(sum( (train_data$Ct - mean(train_data$Ct))^2 )/nrow(train_data))

RMSE_null_test <- sqrt(sum( (test_data$Ct - mean(test_data$Ct))^2 )/nrow(test_data))

print(RMSE_null_train)

#predict(lm_lm, train_data=Ct)
```




2.5. Hyper parameter tuning
```{r}

```

2.6. Model evaluation
     - Regression
       - minimize
         - MSE
         - RMSE
         - Deviance
         - MAE
         - RMSLE
       - maximize
         - R^2
     - classification
       - minimize
         - Misclassifcation
         - Mean per class e rror
         - MSE
         - Cross-entropy -- log loss/deviance
         - Gini index
       - maximize
         - Accuracy
         - Precision
         - Sensitivity / recall
         - Specificity
         - AUC
    
```{r}

```



Tree model
```{r}
tree_model <-  decision_tree() %>% 
  set_args( cost_complexity = tune(), tree_depth = tune(), min_n = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("regression")      
```
Workflow
```{r}
#set workflow
tree_wf <- workflow() %>%
  add_model(tree_model) %>%
  add_recipe(fit_recipe)
```
Tree Model
```{r}
#tuning grid
tree_grid <- dials::grid_regular(cost_complexity(), tree_depth(), min_n(), levels = 5)
#tune the model
tree_tune_res <- tree_wf %>% 
  tune::tune_grid(
    resamples = cv_data,
    grid = tree_grid,
    metrics = yardstick::metric_set(rmse) 
  )
```

```{r}
#see a plot of performance for different tuning parameters
tree_tune_res %>% autoplot()
```

```{r}
# get the tuned model that performs best 
best_tree <- tree_tune_res %>%  select_best(metric = "rmse")
# finalize workflow with best model
best_tree_wf <- tree_wf %>% finalize_workflow(best_tree)
# fitting best performing model
best_tree_fit <- best_tree_wf %>% 
                 fit(data = train_data)
#predicting outcomes for final model
tree_pred <- predict(best_tree_fit, train_data)
```

```{r}
rpart.plot(extract_fit_parsnip(best_tree_fit)$fit)
```

```{r}
#predicted versus observed
plot(tree_pred$.pred,train_data$Ct, xlim =c(0,100), ylim=c(0,100))
abline(a=0,b=1, col = 'red') #45 degree line, along which the results should fall

```

```{r}
#residuals
plot(tree_pred$.pred-train_data$Ct)
abline(a=0,b=0, col = 'red') #straight line, along which the results should fall
```

```{r}
tree_perfomance <- tree_tune_res %>% show_best(n = 5)
print(tree_perfomance)
```

